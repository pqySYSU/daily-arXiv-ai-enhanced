<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 27]
- [cs.CL](#cs.CL) [Total: 7]
- [cs.AI](#cs.AI) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation](https://arxiv.org/abs/2512.16811)
*Jingjing Qian,Boyao Han,Chen Shi,Lei Xiao,Long Yang,Shaoshuai Shi,Li Jiang*

Main category: cs.CV

TL;DR: GeoPredict enhances Vision-Language-Action models with predictive 3D geometric reasoning for better robotic manipulation in spatially demanding tasks.


<details>
  <summary>Details</summary>
Motivation: Current VLA models are reactive and 2D-centric, making them unreliable for tasks requiring precise 3D reasoning and spatial awareness in robotic manipulation.

Method: GeoPredict augments continuous-action policy with predictive kinematic and geometric priors: 1) trajectory-level module encoding motion history and predicting multi-step 3D keypoint trajectories, and 2) predictive 3D Gaussian geometry module forecasting workspace geometry with track-guided refinement along future trajectories. Uses depth-based rendering for training supervision only.

Result: GeoPredict consistently outperforms strong VLA baselines on RoboCasa Human-50, LIBERO, and real-world manipulation tasks, especially in geometry-intensive and spatially demanding scenarios.

Conclusion: GeoPredict successfully addresses the 3D reasoning limitations of current VLA models by incorporating predictive geometric priors, achieving superior performance in complex manipulation tasks while maintaining efficient inference.

Abstract: Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.

</details>


### [2] [DenseBEV: Transforming BEV Grid Cells into 3D Objects](https://arxiv.org/abs/2512.16818)
*Marius Dähling,Sebastian Krebs,J. Marius Zöllner*

Main category: cs.CV

TL;DR: DenseBEV introduces a novel two-stage anchor generation method for multi-camera 3D object detection that uses BEV feature cells directly as anchors instead of random queries, achieving state-of-the-art performance on nuScenes and Waymo datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional BEV-based transformers use random queries as anchors, which are optimized successively. Recent approaches complement or replace these with detections from auxiliary networks. The authors propose a more intuitive and efficient approach by using BEV feature cells directly as anchors, leveraging the dense grid of BEV queries as potential objects.

Method: 1) Uses BEV feature cells directly as anchors in an end-to-end approach; 2) Introduces a two-stage anchor generation method; 3) Applies BEV-based Non-Maximum Suppression to handle scaling issues with many queries; 4) Integrates temporal BEV information by using BEV features from encoders as object queries; 5) Adds hybrid temporal modeling with prior detections for enhanced performance.

Result: Significant improvements on nuScenes dataset in NDS and mAP over baseline, even with sparser BEV grids. Particularly effective for small objects: 3.8% mAP increase for pedestrian detection on nuScenes and 8% increase in LET-mAP on Waymo. Achieves state-of-the-art LET-mAP of 60.7% on Waymo Open dataset, surpassing previous best by 5.4%.

Conclusion: DenseBEV provides an intuitive and efficient approach to multi-camera 3D object detection by using BEV feature cells as anchors, achieving superior performance especially for small objects while maintaining computational efficiency through innovative two-stage processing and gradient flow management.

Abstract: In current research, Bird's-Eye-View (BEV)-based transformers are increasingly utilized for multi-camera 3D object detection. Traditional models often employ random queries as anchors, optimizing them successively. Recent advancements complement or replace these random queries with detections from auxiliary networks. We propose a more intuitive and efficient approach by using BEV feature cells directly as anchors. This end-to-end approach leverages the dense grid of BEV queries, considering each cell as a potential object for the final detection task. As a result, we introduce a novel two-stage anchor generation method specifically designed for multi-camera 3D object detection. To address the scaling issues of attention with a large number of queries, we apply BEV-based Non-Maximum Suppression, allowing gradients to flow only through non-suppressed objects. This ensures efficient training without the need for post-processing. By using BEV features from encoders such as BEVFormer directly as object queries, temporal BEV information is inherently embedded. Building on the temporal BEV information already embedded in our object queries, we introduce a hybrid temporal modeling approach by integrating prior detections to further enhance detection performance. Evaluating our method on the nuScenes dataset shows consistent and significant improvements in NDS and mAP over the baseline, even with sparser BEV grids and therefore fewer initial anchors. It is particularly effective for small objects, enhancing pedestrian detection with a 3.8% mAP increase on nuScenes and an 8% increase in LET-mAP on Waymo. Applying our method, named DenseBEV, to the challenging Waymo Open dataset yields state-of-the-art performance, achieving a LET-mAP of 60.7%, surpassing the previous best by 5.4%. Code is available at https://github.com/mdaehl/DenseBEV.

</details>


### [3] [Next-Generation License Plate Detection and Recognition System using YOLOv8](https://arxiv.org/abs/2512.16826)
*Arslan Amin,Rafia Mumtaz,Muhammad Jawad Bashir,Syed Mohammad Hassan Zaidi*

Main category: cs.CV

TL;DR: YOLOv8 variants (Nano and Small) achieve high precision in license plate and character recognition tasks, with a custom character sequencing method and optimized pipeline for edge deployment in intelligent transportation systems.


<details>
  <summary>Details</summary>
Motivation: Efficient license plate detection and recognition are crucial for traffic management and vehicle surveillance, but consistent real-time accuracy in diverse environments remains challenging. This study aims to advance Intelligent Transportation Systems by improving LPR performance.

Method: The study evaluates YOLOv8 variants (Nano and Small) on LPR and Character Recognition tasks using two distinct datasets. A custom method for character sequencing based on x-axis positions was introduced, and an optimized pipeline combining YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition was proposed.

Result: YOLOv8 Nano achieved precision of 0.964 and mAP50 of 0.918 on LPR task, while YOLOv8 Small achieved precision of 0.92 and mAP50 of 0.91 on Character Recognition task. The custom character sequencing method effectively sequenced detected characters.

Conclusion: The proposed optimized pipeline maintains computational efficiency while ensuring high accuracy, providing a robust foundation for real-world deployment on edge devices in Intelligent Transportation Systems, marking progress toward smarter urban infrastructure.

Abstract: In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.

</details>


### [4] [Radiology Report Generation with Layer-Wise Anatomical Attention](https://arxiv.org/abs/2512.16841)
*Emmanuel D. Muñiz-De-León,Jorge A. Rosales-de-Golferichs,Ana S. Muñoz-Rodríguez,Alejandro I. Trejo-Castro,Eduardo de Avila-Armenta,Antonio Martínez-Torteya*

Main category: cs.CV

TL;DR: A compact image-to-text model generates chest X-ray report findings from single frontal images using frozen DINOv3 ViT encoder and GPT-2 decoder with anatomical attention, achieving significant performance gains over resource-intensive SOTA systems.


<details>
  <summary>Details</summary>
Motivation: Current SOTA radiology report generation systems (MAIRA-2, MedPaLM-M) require large-scale multimodal training, clinical metadata, and multiple imaging views, making them resource-intensive and inaccessible for most clinical settings.

Method: Combines frozen DINOv3 ViT encoder with GPT-2 decoder enhanced by layer-wise anatomical attention using lung/heart segmentation masks through hierarchical Gaussian smoothing, biasing attention toward clinically relevant regions without adding trainable parameters.

Result: Achieved substantial gains on MIMIC-CXR: CheXpert Macro-F1 for 5 key pathologies increased 168% (0.083→0.238), Micro-F1 increased 146% (0.137→0.337), broader 14-observation performance improved 86% (0.170→0.316), and RadGraph F1 rose 9.7%.

Conclusion: Despite small size and purely image-conditioned design, decoder-level anatomical guidance improves spatial grounding and coherence in clinically relevant regions, offering accessible alternative to resource-intensive SOTA systems.

Abstract: Automatic radiology report generation is a promising application of multimodal deep learning, aiming to reduce reporting workload and improve consistency. However, current state-of-the-art (SOTA) systems - such as Multimodal AI for Radiology Applications (MAIRA-2) and Medical Pathways Language Model-Multimodal (MedPaLM-M) - depend on large-scale multimodal training, clinical metadata, and multiple imaging views, making them resource-intensive and inaccessible for most settings. We introduce a compact image-to-text architecture that generates the Findings section of chest X-ray reports from a single frontal image. The model combines a frozen Self-Distillation with No Labels v3 (DINOv3) Vision Transformer (ViT) encoder with a Generative Pre-trained Transformer 2 (GPT-2) decoder enhanced by layer-wise anatomical attention. This mechanism integrates lung and heart segmentation masks through hierarchical Gaussian smoothing, biasing attention toward clinically relevant regions without adding trainable parameters. Evaluated on the official Medical Information Mart for Intensive Care-Chest X-ray (MIMIC-CXR) dataset using Chest Radiograph Expert (CheXpert) and Radiology Graph (RadGraph) metrics, our approach achieved substantial gains: CheXpert Macro-F1 for five key pathologies increased by 168% (0.083 -> 0.238) and Micro-F1 by 146% (0.137 -> 0.337), while broader performance across 14 observations improved by 86% (0.170 -> 0.316). Structural coherence also improved, with RadGraph F1 rising by 9.7%. Despite its small size and purely image-conditioned design, the model demonstrates that decoder-level anatomical guidance improves spatial grounding and enhances coherence in clinically relevant regions. The source code is publicly available at: https://github.com/devMuniz02/UDEM-CXR-Reporting-Thesis-2025.

</details>


### [5] [OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction](https://arxiv.org/abs/2512.16842)
*Yuxin Ray Song,Jinzhou Li,Rao Fu,Devin Murphy,Kaichen Zhou,Rishi Shiv,Yaqi Li,Haoyu Xiong,Crystal Elaine Owens,Yilun Du,Yiyue Luo,Xianyi Cheng,Antonio Torralba,Wojciech Matusik,Paul Pu Liang*

Main category: cs.CV

TL;DR: OpenTouch is the first in-the-wild egocentric full-hand tactile dataset with synchronized video-touch-pose data and text annotations, enabling benchmarks for touch-based perception and action understanding.


<details>
  <summary>Details</summary>
Motivation: Despite the hand being our primary interface with the physical world, current egocentric perception lacks understanding of when, where, and how forcefully hands make contact. There's a gap between visual perception and physical interaction, with no existing in-the-wild datasets aligning first-person video with full-hand touch.

Method: The authors present OpenTouch, a dataset containing 5.1 hours of synchronized video-touch-pose data with 2,900 curated clips and detailed text annotations. They introduce retrieval and classification benchmarks to study how touch grounds perception and action.

Result: Tactile signals provide compact yet powerful cues for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. The dataset enables new benchmarks for multimodal perception.

Conclusion: By releasing this annotated vision-touch-pose dataset and benchmark, the authors aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation, bridging the gap between visual perception and physical interaction.

Abstract: The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.

</details>


### [6] [GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation](https://arxiv.org/abs/2512.16853)
*Amita Kamath,Kai-Wei Chang,Ranjay Krishna,Luke Zettlemoyer,Yushi Hu,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: GenEval benchmark has drifted from human judgment over time, prompting creation of GenEval 2 with better concept coverage and Soft-TIFA evaluation method to reduce future drift.


<details>
  <summary>Details</summary>
Motivation: Address benchmark drift in T2I evaluation where static benchmarks fail to keep up with evolving model capabilities, as shown by GenEval's 17.7% absolute error from human judgment for current models.

Method: Introduce GenEval 2 benchmark with improved coverage of primitive visual concepts and higher compositionality, plus Soft-TIFA evaluation method that combines judgments for visual primitives rather than holistic scoring.

Result: GenEval 2 is more challenging for current models than the original GenEval, and Soft-TIFA shows better alignment with human judgment while being less likely to drift over time compared to holistic judges like VQAScore.

Conclusion: Benchmark drift is a significant problem requiring continual audits; GenEval 2 provides a stronger benchmark but highlights the need for ongoing improvement in T2I evaluation methods.

Abstract: Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.

</details>


### [7] [RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing](https://arxiv.org/abs/2512.16864)
*Tianyuan Qu,Lei Ke,Xiaohang Zhan,Longxiang Tang,Yuqi Liu,Bohao Peng,Bei Yu,Dong Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: RePlan is a plan-then-execute framework for complex instruction-based image editing that uses a vision-language planner to decompose instructions and ground them to target regions, then applies parallel multi-region edits via attention-region injection, outperforming baselines on IV-Complex settings.


<details>
  <summary>Details</summary>
Motivation: Existing instruction-based image editing models struggle with Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes, leading to poor performance in fine-grained grounding and knowledge-intensive edits.

Method: RePlan uses a two-stage framework: 1) A vision-language planner that decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions, enhanced by GRPO-based reinforcement learning; 2) A diffusion editor that applies changes using a training-free attention-region injection mechanism for precise, parallel multi-region edits without iterative inpainting.

Result: RePlan consistently outperforms strong baselines trained on far larger datasets across IV-Complex settings, improving regional precision and overall fidelity. The GRPO-based RL training with only 1K instruction-only examples yields substantial gains in reasoning fidelity and format reliability.

Conclusion: The RePlan framework effectively addresses IV-Complexity in instruction-based image editing through explicit planning and region-aligned execution, demonstrating strong performance on fine-grained grounding and knowledge-intensive edits with the proposed IV-Edit benchmark.

Abstract: Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io

</details>


### [8] [Pixel Seal: Adversarial-only training for invisible image and video watermarking](https://arxiv.org/abs/2512.16874)
*Tomáš Souček,Pierre Fernandez,Hady Elsahar,Sylvestre-Alvise Rebuffi,Valeriu Lacatusu,Tuan Tran,Tom Sander,Alexandre Mourachko*

Main category: cs.CV

TL;DR: Pixel Seal introduces a new state-of-the-art invisible watermarking method that addresses key limitations of existing approaches through adversarial-only training, three-stage training schedule, and high-resolution adaptation techniques.


<details>
  <summary>Details</summary>
Motivation: Current invisible watermarking methods struggle to balance robustness and true imperceptibility, relying on imperfect perceptual losses, suffering from optimization instability, and failing to scale effectively to high-resolution images and videos.

Method: 1) Adversarial-only training paradigm eliminating unreliable pixel-wise imperceptibility losses; 2) Three-stage training schedule decoupling robustness and imperceptibility objectives; 3) High-resolution adaptation using JND-based attenuation and training-time inference simulation to handle resolution scaling.

Result: Pixel Seal achieves clear improvements over state-of-the-art methods in both robustness and imperceptibility across different image types and transformations, and efficiently adapts to video via temporal watermark pooling.

Conclusion: Pixel Seal provides a practical and scalable solution for reliable provenance tracing in real-world image and video settings, setting a new state-of-the-art for invisible watermarking.

Abstract: Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.

</details>


### [9] [Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation](https://arxiv.org/abs/2512.16880)
*Valay Bundele,Mehran Hosseinzadeh,Hendrik P. A. Lensch*

Main category: cs.CV

TL;DR: ReMeDI-SAM3 is a training-free memory-enhanced extension of SAM3 for surgical instrument segmentation that improves occlusion handling and identity recovery through relevance-aware memory filtering, piecewise interpolation, and feature-based re-identification.


<details>
  <summary>Details</summary>
Motivation: Surgical instrument segmentation in endoscopic videos is challenging due to occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a spatio-temporal framework, it suffers from indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions.

Method: Three key components: (1) relevance-aware memory filtering with occlusion-aware memory for storing pre-occlusion frames, (2) piecewise interpolation scheme to expand effective memory capacity, and (3) feature-based re-identification module with temporal voting for post-occlusion identity disambiguation.

Result: Evaluations on EndoVis17 and EndoVis18 show absolute mcIoU improvements of ~7% and ~16% respectively over vanilla SAM3 in zero-shot setting, outperforming even prior training-based approaches.

Conclusion: ReMeDI-SAM3 effectively addresses SAM3's limitations for surgical scenes through memory enhancement techniques, achieving significant performance gains without requiring training, making it suitable for real-world surgical applications.

Abstract: Accurate surgical instrument segmentation in endoscopic videos is crucial for computer-assisted interventions, yet remains challenging due to frequent occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a powerful spatio-temporal framework for video object segmentation, its performance in surgical scenes is limited by indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions. We propose ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3, that addresses these limitations through three components: (i) relevance-aware memory filtering with a dedicated occlusion-aware memory for storing pre-occlusion frames, (ii) a piecewise interpolation scheme that expands the effective memory capacity, and (iii) a feature-based re-identification module with temporal voting for reliable post-occlusion identity disambiguation. Together, these components mitigate error accumulation and enable reliable recovery after occlusions. Evaluations on EndoVis17 and EndoVis18 under a zero-shot setting show absolute mcIoU improvements of around 7% and 16%, respectively, over vanilla SAM3, outperforming even prior training-based approaches. Project page: https://valaybundele.github.io/remedi-sam3/.

</details>


### [10] [M-PhyGs: Multi-Material Object Dynamics from Video](https://arxiv.org/abs/2512.16885)
*Norika Wada,Kohei Yamashita,Ryo Kawahara,Ko Nishino*

Main category: cs.CV

TL;DR: M-PhyGs is a method that estimates material composition and mechanical parameters of complex multi-material objects (like flowers) from video, using Gaussian representations and novel losses.


<details>
  <summary>Details</summary>
Motivation: Existing methods for estimating physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies, but real-world objects are often complex in material composition and geometry.

Method: Multi-material Physical Gaussians (M-PhyGs) jointly segments objects into similar materials and recovers their continuum mechanical parameters from short videos, using cascaded 3D and 2D losses and temporal mini-batching.

Result: Experimental results on the Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components for multi-material physical parameter estimation.

Conclusion: M-PhyGs provides an effective approach for estimating material composition and physical parameters of complex multi-material natural objects from visual data, addressing limitations of existing methods.

Abstract: Knowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.

</details>


### [11] [LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation](https://arxiv.org/abs/2512.16891)
*Haichao Zhang,Yao Lu,Lichen Wang,Yunzhe Li,Daiwei Chen,Yunpeng Xu,Yun Fu*

Main category: cs.CV

TL;DR: LinkedOut is a VLLM-based video representation that extracts world knowledge directly from raw video frames to enable fast, multi-video recommendation without language bottlenecks.


<details>
  <summary>Details</summary>
Motivation: Current Video Large Language Models (VLLMs) face deployment challenges for downstream tasks like video recommendation due to high latency from decode-only generation, lack of multi-video input support, and loss of fine-grained visual details when constrained to language outputs. These limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge.

Method: LinkedOut extracts semantically grounded, knowledge-aware tokens from raw video frames using VLLMs, guided by promptable queries and optional auxiliary modalities. It introduces a cross-layer knowledge fusion Mixture of Experts (MoE) that selects appropriate abstraction levels from rich VLLM features, enabling personalized, interpretable, low-latency recommendation.

Result: LinkedOut achieves state-of-the-art results on standard benchmarks for video recommendation, operating on raw frames without handcrafted labels. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion.

Conclusion: LinkedOut provides a practical path to fully leverage VLLM world-knowledge priors and visual reasoning for downstream vision tasks like recommendation, addressing key deployment challenges through knowledge-aware video representations that bypass language bottlenecks.

Abstract: Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.

</details>


### [12] [FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction](https://arxiv.org/abs/2512.16900)
*Shuyuan Tu,Yueming Pan,Yinming Huang,Xintong Han,Zhen Xing,Qi Dai,Kai Qiu,Chong Luo,Zuxuan Wu*

Main category: cs.CV

TL;DR: FlashPortrait is a video diffusion transformer that generates identity-preserving long portrait animations with 6x speed acceleration using facial expression normalization, dynamic sliding windows, and higher-order latent derivative prediction.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based methods for long portrait animation fail to maintain identity consistency while being computationally slow. There's a need for an efficient solution that preserves identity across long video sequences.

Method: 1) Uses off-the-shelf extractor for identity-agnostic facial expression features; 2) Normalized Facial Expression Block aligns facial features with diffusion latents via mean/variance normalization; 3) Dynamic sliding-window scheme with weighted blending for smooth transitions; 4) Higher-order latent derivatives predict future timesteps to skip denoising steps for 6x acceleration.

Result: FlashPortrait achieves up to 6x acceleration in inference speed while maintaining identity consistency in long portrait animations. Experiments on benchmarks demonstrate effectiveness both qualitatively and quantitatively.

Conclusion: FlashPortrait presents an effective end-to-end solution for identity-preserving long portrait animation with significant speed improvements, addressing key limitations of current diffusion-based methods.

Abstract: Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.

</details>


### [13] [Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection](https://arxiv.org/abs/2512.16905)
*Kaixin Ding,Yang Zhou,Xi Chen,Miao Yang,Jiarong Ou,Rui Chen,Xin Tao,Hengshuang Zhao*

Main category: cs.CV

TL;DR: Alchemist is a meta-gradient-based framework that automatically selects high-quality subsets from large-scale text-image datasets to improve T2I model training efficiency and visual quality.


<details>
  <summary>Details</summary>
Motivation: Current T2I models are limited by low-quality training data from web-crawled and synthetic sources, which cause degraded visual fidelity, unstable training, and inefficient computation. Existing data selection methods rely on manual curation or heuristic scoring, and meta-learning approaches haven't been adapted for image modalities.

Method: Alchemist uses meta-gradient-based framework with two stages: 1) Data rating - trains a lightweight rater to estimate sample influence using gradient information enhanced with multi-granularity perception; 2) Data pruning - uses Shift-Gsampling strategy to select informative subsets for efficient model training.

Result: Experiments on synthetic and web-crawled datasets show Alchemist consistently improves visual quality and downstream performance. Training on Alchemist-selected 50% of data can outperform training on the full dataset.

Conclusion: Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for T2I model training, addressing data quality limitations through automated, gradient-based sample influence assessment and selection.

Abstract: Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose **Alchemist**, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample's influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.

</details>


### [14] [VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization](https://arxiv.org/abs/2512.16906)
*Xiaoyan Cong,Haotian Yang,Angtian Wang,Yizhi Wang,Yiding Yang,Canyu Zhang,Chongyang Ma*

Main category: cs.CV

TL;DR: VIVA is a scalable framework for instruction-based video editing that uses VLM-guided encoding and reward optimization to handle diverse real-world instructions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based video editing methods are limited by training on simple editing operations, creating a generalization gap for complex real-world instructions.

Method: Uses VLM-based instructor for fine-grained instruction encoding and Edit-GRPO (Group Relative Policy Optimization) for reward-based optimization, plus synthetic data generation pipeline.

Result: VIVA achieves superior instruction following, generalization, and editing quality compared to state-of-the-art methods.

Conclusion: The proposed framework effectively addresses the generalization gap in instruction-based video editing through VLM-guided encoding and reward optimization.

Abstract: Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io

</details>


### [15] [Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos](https://arxiv.org/abs/2512.16907)
*Mingfei Chen,Yifan Wang,Zhengqin Li,Homanga Bharadhwaj,Yujin Chen,Chuan Qin,Ziyi Kou,Yuan Tian,Eric Whitmire,Rajinder Sodhi,Hrvoje Benko,Eli Shlizerman,Yue Liu*

Main category: cs.CV

TL;DR: EgoMAN introduces a dataset and model for 3D hand trajectory prediction that integrates semantic reasoning with motion generation, addressing limitations of prior decoupled approaches.


<details>
  <summary>Details</summary>
Motivation: Prior 3D hand trajectory prediction works are limited by datasets that separate motion from semantic supervision and models that weakly connect reasoning with action, creating a gap between understanding and motion generation.

Method: First create EgoMAN dataset with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. Then develop EgoMAN model - a reasoning-to-motion framework linking vision-language reasoning and motion generation via trajectory-token interface with progressive training.

Result: The approach yields accurate and stage-aware trajectories with generalization across real-world scenes, demonstrating improved integration of reasoning with motion dynamics.

Conclusion: EgoMAN addresses limitations of prior work by providing both a comprehensive dataset and a model that effectively links semantic reasoning with 3D hand trajectory prediction, enabling better stage-aware motion generation.

Abstract: Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.

</details>


### [16] [SceneDiff: A Benchmark and Method for Multiview Object Change Detection](https://arxiv.org/abs/2512.16908)
*Yuqun Wu,Chih-hao Lin,Henry Che,Aditi Tiwari,Chuhang Zou,Shenlong Wang,Derek Hoiem*

Main category: cs.CV

TL;DR: SceneDiff introduces a multiview object change detection benchmark with object instance annotations and a training-free method that leverages pretrained 3D, segmentation, and image encoding models to detect added, removed, or moved objects between scene captures.


<details>
  <summary>Details</summary>
Motivation: Detecting object changes between scene captures is important for applications like robotic tidying and construction monitoring, but varying viewpoints can cause false detections. Existing benchmarks lack multiview scenarios with object instance annotations.

Method: A training-free approach that aligns captures in 3D, extracts object regions using pretrained models, and compares spatial and semantic region features to detect changes. Leverages pretrained 3D, segmentation, and image encoding models without additional training.

Result: Outperforms existing approaches by large margins (94% and 37.4% relative AP improvements) on multiview and two-view benchmarks. Introduces SceneDiff Benchmark with 350 diverse video pairs and thousands of changed object annotations.

Conclusion: SceneDiff provides an effective training-free solution for multiview object change detection and establishes the first comprehensive benchmark with object instance annotations, enabling better evaluation and advancement in this important computer vision task.

Abstract: We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.

</details>


### [17] [MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning](https://arxiv.org/abs/2512.16909)
*Yuanchen Ju,Yongyuan Liang,Yen-Jen Wang,Nandiraju Gireesh,Yuanliang Ju,Seungjae Lee,Qiao Gu,Elvis Hsieh,Furong Huang,Koushil Sreenath*

Main category: cs.CV

TL;DR: MomaGraph: A unified scene graph representation for mobile manipulators that integrates spatial-functional relationships and part-level interactivity, with associated dataset, benchmark, and a 7B vision-language model achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Mobile manipulators need semantically rich scene representations that capture object locations, functions, and actionable parts. Prior work separates spatial/functional relations, treats scenes as static, and overlooks task-relevant information.

Method: Introduces MomaGraph unified scene representation, creates MomaGraph-Scenes dataset (large-scale annotated task-driven scene graphs), MomaGraph-Bench evaluation suite, and MomaGraph-R1 (7B vision-language model trained with RL on the dataset).

Result: MomaGraph-R1 achieves 71.6% accuracy on the benchmark (+11.4% over best baseline), demonstrates state-of-the-art results among open-source models, generalizes across public benchmarks, and transfers effectively to real-robot experiments.

Conclusion: MomaGraph provides a comprehensive framework for embodied agents with unified scene representation, supporting both task planning and fine-grained scene understanding, validated through extensive experiments and real-robot deployment.

Abstract: Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.

</details>


### [18] [SFTok: Bridging the Performance Gap in Discrete Tokenizers](https://arxiv.org/abs/2512.16910)
*Qihang Rao,Borui Zhang,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: SFTok is a novel discrete image tokenizer that uses multi-step iterative reconstruction with self-forcing guidance and debias-and-fitting training to achieve state-of-the-art reconstruction quality at high compression rates.


<details>
  <summary>Details</summary>
Motivation: Current discrete tokenizers lag behind continuous ones in multimodal systems despite their natural alignment with autoregressive paradigms, limiting their adoption in high-resolution image generation tasks.

Method: SFTok incorporates a multi-step iterative mechanism with self-forcing guided visual reconstruction and debias-and-fitting training strategy to resolve training-inference inconsistency in the multi-step process.

Result: At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and exceptional performance in class-to-image generation tasks (gFID = 2.29).

Conclusion: SFTok demonstrates that discrete tokenizers can achieve superior reconstruction quality through innovative multi-step iterative mechanisms and specialized training strategies, enabling efficient high-resolution image generation in multimodal systems.

Abstract: Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \textbf{self-forcing guided visual reconstruction} and \textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).

</details>


### [19] [Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation](https://arxiv.org/abs/2512.16913)
*Xin Lin,Meixi Song,Dizhe Zhang,Wenxuan Lu,Haodong Li,Bo Du,Ming-Hsuan Yang,Truong Nguyen,Lu Qi*

Main category: cs.CV

TL;DR: A panoramic metric depth foundation model that generalizes across diverse scene distances using a data-in-the-loop approach with large-scale dataset collection and pseudo-label curation, achieving strong performance and zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: To create a panoramic metric depth foundation model that can handle diverse scene distances and generalize well across different environments (indoor/outdoor, synthetic/real).

Method: Data-in-the-loop paradigm combining public datasets, UE5 synthetic data, text-to-image models, and web panoramic images. Three-stage pseudo-label curation pipeline to reduce domain gaps. DINOv3-Large backbone with plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization for robustness and consistency.

Result: Strong performance on multiple benchmarks (Stanford2D3D, Matterport3D, Deep360) with robust zero-shot generalization and stable metric predictions in diverse real-world scenes.

Conclusion: The proposed panoramic metric depth foundation model effectively generalizes across diverse scene distances through comprehensive data collection and innovative framework design, demonstrating practical applicability in real-world scenarios.

Abstract: In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP\_website/}

</details>


### [20] [StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors](https://arxiv.org/abs/2512.16915)
*Guibao Shen,Yihua Du,Wenhang Ge,Jing He,Chirui Chang,Donghao Zhou,Zhen Yang,Luozhou Wang,Xin Tao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: UniStereo dataset and StereoPilot model for efficient monocular-to-stereo video conversion without depth maps or iterative diffusion sampling.


<details>
  <summary>Details</summary>
Motivation: High demand for stereo video content but current production is costly/complex; existing DWI pipeline has error propagation, depth ambiguity, and format inconsistency issues between parallel and converged stereo configurations.

Method: Created UniStereo dataset (first large-scale unified dataset for stereo video conversion covering both formats). Proposed StereoPilot model: efficient feed-forward model directly synthesizes target view without explicit depth maps or iterative diffusion sampling, with learnable domain switcher and cycle consistency loss.

Result: StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency.

Conclusion: UniStereo dataset enables fair benchmarking and robust training; StereoPilot adapts seamlessly to different stereo formats with improved consistency, offering efficient solution for monocular-to-stereo video conversion.

Abstract: The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.

</details>


### [21] [AdaTooler-V: Adaptive Tool-Use for Images and Videos](https://arxiv.org/abs/2512.16918)
*Chaoyang Wang,Kaituo Feng,Dongyang Chen,Zhongyu Wang,Zhixun Li,Sicheng Gao,Meng Meng,Xu Zhou,Manyuan Zhang,Yuzhang Shang,Xiangyu Yue*

Main category: cs.CV

TL;DR: AdaTooler-V is a multimodal LLM that adaptively decides when to use vision tools, avoiding unnecessary tool invocations through reinforcement learning with adaptive reward scaling based on Tool Benefit Scores.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs often invoke vision tools unnecessarily, increasing inference overhead and degrading performance. The authors aim to create a model that can intelligently determine when visual problems truly require tool assistance.

Method: 1) AT-GRPO reinforcement learning algorithm that adaptively adjusts reward scales based on Tool Benefit Scores; 2) Two datasets: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL training across single-image, multi-image, and video data.

Result: AdaTooler-V outperforms existing methods across twelve benchmarks, with the 7B version achieving 89.8% accuracy on V* benchmark, surpassing commercial models GPT-4o and Gemini 1.5 Pro.

Conclusion: The proposed adaptive tool-use approach effectively reduces unnecessary tool invocations while maintaining strong reasoning capabilities, demonstrating superior performance over existing methods including proprietary commercial models.

Abstract: Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.

</details>


### [22] [DVGT: Driving Visual Geometry Transformer](https://arxiv.org/abs/2512.16919)
*Sicheng Zuo,Zixun Xie,Wenzhao Zheng,Shaoqing Xu,Fang Li,Shengyin Jiang,Long Chen,Zhi-Xin Yang,Jiwen Lu*

Main category: cs.CV

TL;DR: DVGT is a transformer-based model that reconstructs dense 3D point maps from unposed multi-view driving videos without explicit camera parameters or geometric priors.


<details>
  <summary>Details</summary>
Motivation: There's a lack of driving-targeted dense geometry perception models that can adapt to different scenarios and camera configurations without relying on precise camera parameters.

Method: Uses DINO backbone for visual feature extraction, then employs alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations. Multiple heads decode global point maps and ego poses.

Result: Significantly outperforms existing models on various driving scenarios, trained on mixture of nuScenes, OpenScene, Waymo, KITTI, and DDAD datasets.

Conclusion: DVGT enables flexible 3D geometry reconstruction from arbitrary camera configurations without explicit geometric priors, directly predicting metric-scaled geometry from image sequences.

Abstract: Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.

</details>


### [23] [EasyV2V: A High-quality Instruction-based Video Editing Framework](https://arxiv.org/abs/2512.16920)
*Jinjie Mai,Chaoyang Wang,Guocheng Gordon Qian,Willi Menapace,Sergey Tulyakov,Bernard Ghanem,Peter Wonka,Ashkan Mirzaei*

Main category: cs.CV

TL;DR: EasyV2V is a simple yet effective framework for instruction-based video editing that addresses challenges in consistency, control, and generalization through innovations in data composition, model architecture, and control mechanisms.


<details>
  <summary>Details</summary>
Motivation: Video editing lags behind image editing due to challenges in maintaining temporal consistency, providing flexible control, and generalizing across different editing scenarios. The authors aim to create a comprehensive framework that addresses these limitations.

Method: Three-pronged approach: 1) Data composition using existing experts with fast inverses, single-frame supervision, pseudo pairs with shared affine motion, dense-captioned clips, and transition supervision; 2) Model design leveraging pretrained text-to-video models with simple sequence concatenation and light LoRA fine-tuning; 3) Control unification via single mask mechanism supporting video+text, video+mask+text, and video+mask+reference+text inputs.

Result: EasyV2V achieves state-of-the-art video editing results, surpassing both concurrent research systems and commercial solutions, while supporting flexible input combinations for diverse editing scenarios.

Conclusion: The proposed framework demonstrates that a simplified design with careful data composition and unified control mechanisms can effectively address video editing challenges, establishing a strong baseline for instruction-based video editing.

Abstract: While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/

</details>


### [24] [Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification](https://arxiv.org/abs/2512.16921)
*Qihao Liu,Chengzhi Mao,Yaojie Liu,Alan Yuille,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: AuditDM is an automated framework that discovers and fixes multimodal LLM failure modes by training an auditor model to generate challenging questions and counterfactual images that maximize disagreement among target models.


<details>
  <summary>Details</summary>
Motivation: Current multimodal LLM evaluation methods lack interpretability and fail to reveal significant capability gaps between models, creating a need for better diagnostic tools.

Method: Fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models, uncovering diverse failure types.

Result: Applied to models like Gemma-3 and PaliGemma-2, discovered 20+ distinct failure types; fine-tuning on these discoveries improved all models across 16 benchmarks, enabling a 3B model to surpass its 28B counterpart.

Conclusion: As data scaling hits diminishing returns, targeted model auditing offers an effective path for model diagnosis and improvement, providing interpretable failure analysis and annotation-free data for rectification.

Abstract: Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.

</details>


### [25] [Next-Embedding Prediction Makes Strong Vision Learners](https://arxiv.org/abs/2512.16922)
*Sihan Xu,Ziqiao Ma,Wenhao Chai,Xuweiyi Chen,Weiyang Jin,Joyce Chai,Saining Xie,Stella X. Yu*

Main category: cs.CV

TL;DR: NEPA proposes a generative pretraining approach for vision where models predict future patch embeddings from past ones using causal masking, achieving strong performance on ImageNet and ADE20K without complex objectives.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the success of generative pretraining in NLP and asks whether similar principles can work for visual self-supervised learning. Instead of learning features for downstream tasks, they aim to learn models that generate embeddings to perform predictive tasks directly.

Method: NEPA (Next-Embedding Predictive Autoregression) trains models to predict future patch embeddings conditioned on past ones using causal masking and stop gradient. This is a simple Transformer pretrained on ImageNet-1k with next embedding prediction as the sole objective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads.

Result: NEPA achieves 83.8% top-1 accuracy on ImageNet-1K with ViT-B backbone and 85.3% with ViT-L after fine-tuning. It also transfers effectively to semantic segmentation on ADE20K, demonstrating strong performance across tasks.

Conclusion: Generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning, shifting from learning representations to learning models that can perform predictive tasks directly.

Abstract: Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.

</details>


### [26] [Generative Refocusing: Flexible Defocus Control from a Single Image](https://arxiv.org/abs/2512.16923)
*Chun-Wei Tuan Mu,Jia-Bin Huang,Yu-Lun Liu*

Main category: cs.CV

TL;DR: Generative Refocusing: A two-step method using DeblurNet and BokehNet for single-image refocusing with semi-supervised training combining synthetic and real data.


<details>
  <summary>Details</summary>
Motivation: Depth-of-field control is essential in photography but difficult to achieve with single images. Current methods require all-in-focus inputs, rely on synthetic data, and have limited aperture control.

Method: Two-step process: 1) DeblurNet recovers all-in-focus images from various inputs, 2) BokehNet creates controllable bokeh. Uses semi-supervised training combining synthetic paired data with unpaired real bokeh images using EXIF metadata.

Result: Achieves state-of-the-art performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Enables text-guided adjustments and custom aperture shapes.

Conclusion: Generative Refocusing overcomes limitations of current methods by using semi-supervised training with real data, providing better optical characteristics and more control over refocusing effects.

Abstract: Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.

</details>


### [27] [The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text](https://arxiv.org/abs/2512.16924)
*Hanlin Wang,Hao Ouyang,Qiuyu Wang,Yue Yu,Yihao Meng,Wen Wang,Ka Leong Cheng,Shuailei Ma,Qingyan Bai,Yixuan Li,Cheng Chen,Yanhong Zeng,Xing Zhu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: WorldCanvas is a multimodal framework for generating controllable world events using text prompts, motion trajectories, and reference images to create coherent videos with multi-agent interactions and object consistency.


<details>
  <summary>Details</summary>
Motivation: To advance world models from passive predictors to interactive, user-shaped simulators by enabling rich, user-directed simulation of world events that goes beyond text-only approaches and existing trajectory-controlled methods.

Method: Combines multimodal inputs: trajectories (encoding motion, timing, visibility), natural language (for semantic intent), and reference images (for visual grounding of object identity) to generate coherent, controllable events.

Result: Generates videos demonstrating temporal coherence and emergent consistency, preserving object identity and scene despite temporary disappearance, supporting multi-agent interactions, object entry/exit, reference-guided appearance, and counterintuitive events.

Conclusion: WorldCanvas enables expressive world events generation, advancing world models from passive predictors to interactive, user-shaped simulators that support rich, controllable simulation.

Abstract: We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [28] [Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs](https://arxiv.org/abs/2512.16814)
*William English,Dominic Simon,Sumit Kumar Jha,Rickard Ewetz*

Main category: cs.CL

TL;DR: GraFT framework improves NL-to-temporal-logic translation by reducing solution space complexity through grammar-constrained token prediction, achieving significant accuracy gains over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current NL-to-temporal-logic translation approaches struggle with accurate atomic proposition lifting, handling co-references, and learning from limited data, requiring a more efficient framework.

Method: Grammar Forced Translation (GraFT) reduces task complexity by restricting valid output tokens at each step using problem-specific properties, rather than predicting from full vocabulary, with theoretical justification for learning efficiency.

Result: GraFT improves end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average across CW, GLTL, and Navi benchmarks compared to state-of-the-art approaches.

Conclusion: GraFT provides an effective framework for NL-to-temporal-logic translation by exploiting problem-specific properties to reduce solution space complexity, leading to significant accuracy improvements in both in-domain and out-of-domain scenarios.

Abstract: Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.

</details>


### [29] [What Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels](https://arxiv.org/abs/2512.16832)
*Aditya Yadavalli,Tiago Pimentel,Tamar I Regev,Ethan Wilcox,Alex Warstadt*

Main category: cs.CL

TL;DR: This paper proposes an information-theoretic method to quantify how much information is conveyed by prosody (speech melody) alone versus text, and what that information specifically communicates about dimensions like sarcasm, emotion, and questionhood.


<details>
  <summary>Details</summary>
Motivation: Prosody conveys critical information not captured by text alone, but there's a need to systematically quantify how much information prosody provides and what specific dimensions of meaning it communicates about.

Method: The authors use large speech and language models to estimate mutual information between specific meaning dimensions (emotion, sarcasm, questionhood) and communication channels (audio vs text). They apply this approach to speech from television and podcasts.

Result: For sarcasm and emotion, the audio channel (and by implication prosody) transmits over an order of magnitude more information than text alone when long-term context is unavailable. For questionhood, prosody provides comparatively less additional information.

Conclusion: The paper outlines a program to apply this information-theoretic approach to more dimensions of meaning, communication channels, and languages, demonstrating a systematic way to quantify prosodic information.

Abstract: Prosody -- the melody of speech -- conveys critical information often not captured by the words or text of a message. In this paper, we propose an information-theoretic approach to quantify how much information is expressed by prosody alone and not by text, and crucially, what that information is about. Our approach applies large speech and language models to estimate the mutual information between a particular dimension of an utterance's meaning (e.g., its emotion) and any of its communication channels (e.g., audio or text). We then use this approach to quantify how much information is conveyed by audio and text about sarcasm, emotion, and questionhood, using speech from television and podcasts. We find that for sarcasm and emotion the audio channel -- and by implication the prosodic channel -- transmits over an order of magnitude more information about these features than the text channel alone, at least when long-term context beyond the current sentence is unavailable. For questionhood, prosody provides comparatively less additional information. We conclude by outlining a program applying our approach to more dimensions of meaning, communication channels, and languages.

</details>


### [30] [LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference](https://arxiv.org/abs/2512.16843)
*Harsh Vardhan Bansal*

Main category: cs.CL

TL;DR: LLMCache is a layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences, achieving up to 3.1× speedup with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Transformer models have high inference latency that limits real-time and large-scale deployment. Existing caching mechanisms like token-level key-value caches are limited in scope and applicability, creating a need for more general and effective caching solutions.

Method: LLMCache uses a layer-wise caching framework that reuses intermediate activations based on semantic similarity of input sequences. It features a lightweight fingerprinting mechanism for matching semantically similar inputs, adaptive eviction strategies to manage cache staleness, and operates across both encoder and decoder architectures at arbitrary transformer layers.

Result: Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA datasets show up to 3.1× speedup in inference time with less than 0.5% accuracy degradation, demonstrating practical effectiveness.

Conclusion: LLMCache is a practical and general-purpose solution for optimizing transformer inference in real-world applications, offering model-agnostic acceleration with minimal accuracy impact.

Abstract: Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications

</details>


### [31] [AdaSearch: Balancing Parametric Knowledge and Search in Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2512.16883)
*Tzu-Han Lin,Wei-Lin Chen,Chen-An Li,Hung-yi Lee,Yun-Nung Chen,Yu Meng*

Main category: cs.CL

TL;DR: AdaSearch is a two-stage RL framework that improves LLM search agents by making search invocation decisions explicit and interpretable, reducing unnecessary searches while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Current search agents overuse search engines (costly, risky) or rely too much on parametric knowledge (hallucination risk). Existing methods penalize search calls but suffer from reward engineering issues, ambiguous credit assignment, and conflate necessary/unnecessary searches. There's a need for agents that adaptively balance parametric knowledge with external search, with transparent decision-making for high-stakes domains.

Method: AdaSearch: a two-stage, outcome-driven RL framework that disentangles problem solving from search invocation decisions. First stage focuses on solving problems, second stage makes explicit, interpretable decisions about whether to invoke search. This creates transparency in the decision process.

Result: Experiments across multiple model families and sizes show AdaSearch substantially improves knowledge-boundary awareness, reduces unnecessary search calls, preserves strong task performance, and offers more transparent, interpretable decision behaviors compared to methods like Search-R1.

Conclusion: AdaSearch addresses limitations of prior search agents by providing an explicit, interpretable framework for adaptive search invocation, balancing parametric knowledge with external search while maintaining transparency crucial for high-stakes applications.

Abstract: Equipping large language models (LLMs) with search engines via reinforcement learning (RL) has emerged as an effective approach for building search agents. However, overreliance on search introduces unnecessary cost and risks exposure to noisy or malicious content, while relying solely on parametric knowledge risks hallucination. The central challenge is to develop agents that adaptively balance parametric knowledge with external search, invoking search only when necessary. Prior work mitigates search overuse by shaping rewards around the number of tool calls. However, these penalties require substantial reward engineering, provide ambiguous credit assignment, and can be exploited by agents that superficially reduce calls. Moreover, evaluating performance solely through call counts conflates necessary and unnecessary search, obscuring the measurement of true adaptive behavior. To address these limitations, we first quantify the self-knowledge awareness of existing search agents via an F1-based decision metric, revealing that methods such as Search-R1 often overlook readily available parametric knowledge. Motivated by these findings, we propose AdaSearch, a simple two-stage, outcome-driven RL framework that disentangles problem solving from the decision of whether to invoke search, and makes this decision process explicit and interpretable. This transparency is crucial for high-stakes domains such as finance and medical question answering, yet is largely neglected by prior approaches. Experiments across multiple model families and sizes demonstrate that AdaSearch substantially improves knowledge-boundary awareness, reduces unnecessary search calls, preserves strong task performance, and offers more transparent, interpretable decision behaviors.

</details>


### [32] [Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image](https://arxiv.org/abs/2512.16899)
*Yushi Hu,Reyhane Askari-Hemmat,Melissa Hall,Emily Dinan,Luke Zettlemoyer,Marjan Ghazvininejad*

Main category: cs.CL

TL;DR: MMRB2 is the first comprehensive benchmark for multimodal reward models, covering 4 tasks with 1,000 expert-annotated preference pairs, revealing current models achieve 64-80% accuracy vs >90% for humans.


<details>
  <summary>Details</summary>
Motivation: Reward models are crucial for training LLMs but remain underexplored for multimodal (image+text) models, creating a need for comprehensive evaluation benchmarks.

Method: Created MMRB2 benchmark with: 1) practical challenging prompts, 2) responses from SOTA models/agents, 3) expert-annotated preference pairs using ensemble filtering strategy across 4 multimodal tasks.

Result: Gemini 3 Pro achieves 75-80% accuracy, GPT-5/Gemini 2.5 Pro reach 66-75%, GPT-4o gets 59%, Qwen3-VL-32B matches Gemini 2.5 Flash at 64%, all significantly below human performance (>90%).

Conclusion: MMRB2 enables systematic evaluation of multimodal reward models, reveals significant performance gaps between current models and humans, and provides insights for future improvements in reward modeling.

Abstract: Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning ("thinking-with-images"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.

</details>


### [33] [In-Context Algebra](https://arxiv.org/abs/2512.16902)
*Eric Todd,Jannik Brinkmann,Rohit Gandikota,David Bau*

Main category: cs.CL

TL;DR: Transformers develop symbolic reasoning mechanisms when trained on arithmetic with variable symbols, learning commutative copying, identity recognition, and closure-based cancellation, enabling generalization to unseen algebraic groups.


<details>
  <summary>Details</summary>
Motivation: Prior work shows transformers develop geometric embeddings for arithmetic with fixed-symbol meanings, but this study investigates how they handle variable symbols whose meanings change across sequences, testing their ability to reason symbolically in-context.

Method: Created a new task where symbol-to-algebraic-group-element assignments vary per sequence, trained transformers on this task, developed targeted data distributions for causal testing, and isolated learned mechanisms through systematic analysis.

Result: Transformers achieved near-perfect accuracy on the variable-symbol task and generalized to unseen algebraic groups. Three consistent mechanisms were identified: commutative copying (dedicated head copies answers), identity element recognition, and closure-based cancellation.

Conclusion: Unlike fixed-symbol settings that produce geometric embeddings, variable-symbol training leads transformers to develop symbolic reasoning mechanisms, demonstrating their capacity for abstract in-context reasoning without fixed token meanings.

Abstract: We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.

</details>


### [34] [Constructive Circuit Amplification: Improving Math Reasoning in LLMs via Targeted Sub-Network Updates](https://arxiv.org/abs/2512.16914)
*Nikhil Prakash,Donghao Ren,Dominik Moritz,Yannick Assogba*

Main category: cs.CL

TL;DR: Constructive Circuit Amplification identifies and updates only sparse task-specific circuits in LLMs, improving math reasoning by up to +11.4% while modifying <2% of components with minimal impact on other abilities.


<details>
  <summary>Details</summary>
Motivation: Prior research shows LLMs have sparse subnetworks (circuits) for specific tasks, and fine-tuning often strengthens existing circuits. This suggests direct intervention on these circuits could enable precise, task-targeted updates without affecting other capabilities.

Method: Constructive Circuit Amplification identifies pivotal tokens from model reasoning traces and model components responsible for desired tasks, then updates only those specific components rather than the entire model.

Result: Applied to mathematical reasoning, the method improves accuracy by up to +11.4% across multiple models while modifying as little as 1.59% of model components. Minimal impact on other abilities was observed on MMLU, TriviaQA, and TruthfulQA benchmarks.

Conclusion: Targeted capabilities can be reliably enhanced by selectively updating a sparse set of model components, demonstrating the feasibility of precise, task-specific interventions in LLMs without compromising other abilities.

Abstract: Prior studies investigating the internal workings of LLMs have uncovered sparse subnetworks, often referred to as circuits, that are responsible for performing specific tasks. Additionally, it has been shown that model performance improvement through fine-tuning often results from the strengthening of existing circuits in the model. Taken together, these findings suggest the possibility of intervening directly on such circuits to make precise, task-targeted updates. Motivated by these findings, we propose a novel method called Constructive Circuit Amplification which identifies pivotal tokens from model reasoning traces as well as model components responsible for the desired task, and updates only those components. Applied to mathematical reasoning, it improves accuracy by up to +11.4% across multiple models while modifying as little as 1.59% of model components, with minimal impact on other abilities as measured by MMLU, TriviaQA, and TruthfulQA. These results demonstrate that targeted capabilities can be reliably enhanced by selectively updating a sparse set of model components.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [35] [Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning](https://arxiv.org/abs/2512.16917)
*Qihao Liu,Luoxin Ye,Wufei Ma,Yu-Cheng Chou,Alan Yuille*

Main category: cs.AI

TL;DR: Generative Adversarial Reasoner (GAR) is an adversarial reinforcement learning framework that co-evolves an LLM reasoner and discriminator to improve mathematical reasoning by providing dense step-level rewards.


<details>
  <summary>Details</summary>
Motivation: LLMs with explicit reasoning capabilities still make process errors like incorrect calculations, brittle logic, and superficially plausible but invalid steps, despite excelling at mathematical reasoning.

Method: GAR uses on-policy joint training with adversarial RL, where a compute-efficient review schedule partitions reasoning chains into logical slices, and a discriminator evaluates each slice's soundness with structured justifications. The reasoner is rewarded for consistent steps yielding correct answers, while the discriminator is rewarded for correctly detecting errors.

Result: On AIME24, improved DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The method provides consistent gains across mathematical benchmarks over standard RL post-training baselines.

Conclusion: GAR enhances LLM reasoning quality through dense, well-calibrated step-level rewards that improve credit assignment and sample efficiency, with modular discriminator enabling flexible reward shaping for various objectives like teacher distillation and preference alignment.

Abstract: Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.

</details>


### [36] [TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge](https://arxiv.org/abs/2512.16855)
*Khurram Khalil,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: TOGGLE is a novel LLM compression framework that uses Signal Temporal Logic to formally specify and enforce linguistic properties during compression, achieving up to 3.3x FLOPs reduction and 68.8% size reduction while maintaining formal guarantees.


<details>
  <summary>Details</summary>
Motivation: LLMs require substantial computational resources that limit deployment on resource-constrained edge devices, and existing compression techniques often degrade linguistic properties without formal guarantees for preserving model behavior.

Method: TOGGLE uses Signal Temporal Logic (STL) to formally specify linguistic properties and employs STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that satisfy linguistic constraints without retraining.

Result: Evaluated on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), TOGGLE achieved up to 3.3x reduction in computational costs (FLOPs) and up to 68.8% reduction in model size while satisfying all specified linguistic properties.

Conclusion: TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware with formal guarantees for preserving linguistic properties.

Abstract: Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.

</details>


### [37] [Distributional AGI Safety](https://arxiv.org/abs/2512.16856)
*Nenad Tomašev,Matija Franklin,Julian Jacobs,Sébastien Krier,Simon Osindero*

Main category: cs.AI

TL;DR: The paper argues that AI safety research should consider "patchwork AGI" scenarios where general intelligence emerges from coordinated groups of sub-AGI agents, and proposes a framework of virtual agentic sandbox economies with market mechanisms for collective risk mitigation.


<details>
  <summary>Details</summary>
Motivation: Current AI safety research focuses on individual AI systems assuming monolithic AGI emergence, but overlooks the alternative "patchwork AGI" hypothesis where general capabilities emerge from coordinated groups of specialized sub-AGI agents. This gap needs urgent attention given rapid deployment of advanced AI agents with coordination capabilities.

Method: Proposes a framework for distributional AGI safety that moves beyond individual agent alignment. Centers on designing virtual agentic sandbox economies (impermeable or semi-permeable) where agent-to-agent transactions are governed by robust market mechanisms, coupled with auditability, reputation management, and oversight systems.

Result: The paper presents a conceptual framework rather than empirical results. It establishes the need for considering patchwork AGI scenarios and outlines a systematic approach to address collective risks through economic and governance mechanisms in controlled agent environments.

Conclusion: The patchwork AGI hypothesis deserves serious consideration in AI safety research. A distributional safety framework with virtual agentic sandbox economies and market-based governance mechanisms is necessary to mitigate collective risks from coordinated groups of AI agents, complementing existing individual agent alignment approaches.

Abstract: AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.

</details>


### [38] [The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI](https://arxiv.org/abs/2512.16873)
*Otman A. Basir*

Main category: cs.AI

TL;DR: The Social Responsibility Stack (SRS) is a six-layer architectural framework that embeds societal values into AI systems through explicit constraints, safeguards, and governance processes, treating responsibility as a closed-loop supervisory control problem.


<details>
  <summary>Details</summary>
Motivation: Current responsible AI and governance efforts provide normative principles but lack enforceable engineering mechanisms that operate throughout the AI system lifecycle, creating a gap between ethical guidelines and practical implementation.

Method: Introduces the Social Responsibility Stack (SRS) - a six-layer architectural framework with constraint-based formulation, safety-envelope and feedback interpretations, and continuous monitoring of fairness, autonomy, cognitive burden, and explanation quality.

Result: Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems demonstrate how SRS translates normative objectives into actionable engineering and operational controls.

Conclusion: SRS bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems through a unified framework that integrates design-time safeguards with runtime monitoring and institutional oversight.

Abstract: Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.

</details>
