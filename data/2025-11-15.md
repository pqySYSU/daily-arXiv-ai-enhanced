<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals](https://arxiv.org/abs/2511.10615)
*Shruti Singh Baghel,Yash Pratap Singh Rathore,Sushovan Jena,Anurag Pradhan,Amit Shukla,Arnav Bhavsar,Pawan Goyal*

Main category: cs.CV

TL;DR: This paper evaluates SmolVLM2 variants (500M and 2.2B parameters) for video description accessibility, introducing two novel BLV-focused evaluation frameworks and testing prompt strategies and mobile deployment performance.


<details>
  <summary>Details</summary>
Motivation: Large VLMs have high resource demands that hinder practical use for BLV users who need detailed, context-aware video descriptions, creating an accessibility gap.

Method: Evaluated SmolVLM2 variants on AVCaps (outdoor) and Charades (indoor) datasets using two novel BLV accessibility frameworks: Multi-Context BLV Framework and Navigational Assistance Framework. Also tested four prompt strategies and deployed models on smartphones with FP32 and INT8 precision variants.

Result: The study systematically assessed how model size affects accessibility-focused description quality and real-world mobile performance constraints.

Conclusion: Smaller VLMs can provide practical accessibility solutions for BLV users while maintaining quality, with optimized deployment strategies enabling mobile usage.

Abstract: Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.

</details>


### [2] [One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models](https://arxiv.org/abs/2511.10629)
*Aleksandr Razin,Danil Kazantsev,Ilya Makarov*

Main category: cs.CV

TL;DR: LUA is a lightweight module that performs super-resolution directly on latent codes before VAE decoding, enabling high-resolution image generation without modifying the base diffusion model or adding extra diffusion stages.


<details>
  <summary>Details</summary>
Motivation: Diffusion models face challenges in scaling beyond training resolutions - direct high-resolution sampling is slow/expensive, while post-hoc image super-resolution introduces artifacts and latency by operating after decoding.

Method: LUA uses a shared Swin-style backbone with scale-specific pixel-shuffle heads to perform super-resolution in latent space before the final VAE decoding step, requiring only a single feed-forward pass.

Result: LUA achieves comparable perceptual quality to pixel-space SR with nearly 3x lower decoding and upscaling time (only +0.42s for 1024px from 512px vs 1.87s for pixel-space SR), and generalizes well across different VAEs.

Conclusion: LUA provides a practical and efficient path to scalable, high-fidelity image synthesis in diffusion pipelines, closely matching native high-resolution generation while being easy to deploy without retraining.

Abstract: Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines.

</details>


### [3] [Depth Anything 3: Recovering the Visual Space from Any Views](https://arxiv.org/abs/2511.10647)
*Haotong Lin,Sili Chen,Junhao Liew,Donny Y. Chen,Zhenyu Li,Guang Shi,Jiashi Feng,Bingyi Kang*

Main category: cs.CV

TL;DR: Depth Anything 3 (DA3) is a simple yet powerful model that predicts consistent geometry from multiple visual inputs using a plain transformer backbone and single depth-ray prediction, achieving state-of-the-art performance across camera pose estimation, multi-view geometry, and visual rendering tasks.


<details>
  <summary>Details</summary>
Motivation: To develop a minimal yet effective model for spatially consistent geometry prediction from arbitrary visual inputs, eliminating the need for architectural specialization and complex multi-task learning while maintaining high performance.

Method: Uses a single plain transformer (vanilla DINO encoder) as backbone without architectural modifications, employs a singular depth-ray prediction target to avoid multi-task complexity, and implements a teacher-student training paradigm.

Result: Sets new SOTA on visual geometry benchmark: 44.3% improvement in camera pose accuracy and 25.1% improvement in geometric accuracy over prior SOTA VGGT, while also outperforming Depth Anything 2 in monocular depth estimation.

Conclusion: DA3 demonstrates that minimal modeling with plain transformers and simple prediction targets can achieve state-of-the-art performance in visual geometry tasks, establishing new benchmarks in camera pose estimation, any-view geometry, and visual rendering.

Abstract: We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.

</details>


### [4] [Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling](https://arxiv.org/abs/2511.10648)
*Jiahao Wang,Weiye Xu,Aijun Yang,Wengang Zhou,Lewei Lu,Houqiang Li,Xiaohua Wang,Jinguo Zhu*

Main category: cs.CV

TL;DR: Self-Consistency Sampling (SCS) addresses the problem of unfaithful reasoning in multimodal LLMs by introducing visual perturbations and trajectory resampling to generate consistency scores that down-weight unreliable reasoning paths during RL training.


<details>
  <summary>Details</summary>
Motivation: Outcome-reward RL in multimodal reasoning faces a critical flaw where incorrect reasoning chains that happen to guess the right answer receive the same reward as genuine reasoning, undermining training effectiveness.

Method: SCS introduces small visual perturbations and performs repeated truncation/resampling of initial trajectories, measuring agreement among resulting trajectories to compute a differentiable consistency score that penalizes unreliable reasoning paths.

Result: SCS improves accuracy by up to 7.7 percentage points on six multimodal benchmarks when integrated with RLOO, GRPO, and REINFORCE++ methods, with negligible computational overhead. Benefits generalize across different model sizes (3B-8B).

Conclusion: SCS provides a simple, general solution to the unfaithful reasoning problem in outcome-reward RL for multimodal LLMs, significantly improving reasoning reliability without substantial computational cost.

Abstract: Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [5] [Know Your Limits: Entropy Estimation Modeling for Compression and Generalization](https://arxiv.org/abs/2511.10618)
*Benjamin L. Badger,Matthew Neligeorge*

Main category: cs.CL

TL;DR: The paper introduces encoder-augmented causal decoder models that achieve better compression than causal transformers and shows that training models to approach estimated per-token entropies improves generalization compared to models trained without entropy considerations.


<details>
  <summary>Details</summary>
Motivation: Current causal language models are computationally infeasible for accurately estimating language entropy, and there's a need for more efficient architectures that can approach the fundamental limits of language compression while improving generalization.

Method: Developed encoder-augmented causal decoder model architectures that exhibit superior training efficiency. Used these models to obtain per-token entropy estimates and trained models to approach but not exceed these estimated entropies.

Result: The proposed architectures achieve higher compression than causal transformers even when trained on modest hardware. Models trained to approach estimated per-token entropies demonstrate greater generalization than those trained without entropy considerations.

Conclusion: Considering language entropy during training leads to better generalization, and encoder-augmented causal decoder models provide a computationally feasible approach to approaching the fundamental limits of language compression.

Abstract: Language prediction is constrained by informational entropy intrinsic to language, such that there exists a limit to how accurate any language model can become and equivalently a lower bound to language compression. The most efficient language compression algorithms today are causal (next token prediction) large language models, but the use of these models to form accurate estimates of language entropy is currently computationally infeasible. We introduce encoder-augmented causal decoder model architectures that exhibit superior training efficiency characteristics and achieve higher compression than causal transformers even when trained on modest hardware. We demonstrate how entropy estimates can be obtained on a per-token basis, and show that the generalization of models trained to approach the entropy of their training data necessarily exceeds the generalization of models trained to minimize loss beyond this value. We show empirically that causal models trained to approach but not exceed estimated per-token entropies exhibit greater generalization than models trained without taking entropy into account.

</details>


### [6] [SSR: Socratic Self-Refine for Large Language Model Reasoning](https://arxiv.org/abs/2511.10621)
*Haizhou Shi,Ye Liu,Bo Pang,Zeyu Leo Liu,Hao Wang,Silvio Savarese,Caiming Xiong,Yingbo Zhou,Semih Yavuz*

Main category: cs.CL

TL;DR: SSR is a novel framework that improves LLM reasoning by decomposing responses into verifiable sub-questions and answers, enabling step-level confidence estimation and iterative refinement of unreliable steps.


<details>
  <summary>Details</summary>
Motivation: Existing test-time frameworks for LLMs rely on coarse self-verification and self-correction, limiting their effectiveness on complex reasoning tasks that require fine-grained evaluation and precise refinement.

Method: SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, performs step-level confidence estimation through controlled re-solving and self-consistency checks, and iteratively refines unreliable steps.

Result: Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines.

Conclusion: SSR provides both performance improvements and a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs, making reasoning chains more accurate and interpretable.

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.

</details>


### [7] [Instella: Fully Open Language Models with Stellar Performance](https://arxiv.org/abs/2511.10628)
*Jiang Liu,Jialian Wu,Xiaodong Yu,Yusheng Su,Prakamya Mishra,Gowtham Ramesh,Sudhanshu Ranjan,Chaitanya Manem,Ximeng Sun,Ze Wang,Pratik Prabhanjan Brahma,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: Instella is a family of fully open 3B parameter language models trained on open data using AMD GPUs, achieving SOTA results among fully open models and competing with leading open-weight models of similar size, with specialized variants for long contexts and mathematical reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the limitation that most high-performing LLMs remain closed-source or partially open, restricting transparency and reproducibility in language modeling research.

Method: Large-scale pre-training on openly available data, general-purpose instruction tuning, and alignment with human preferences using AMD Instinct MI300X GPUs, with specialized variants created through supervised fine-tuning and reinforcement learning for specific tasks.

Result: Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size, despite using fewer pre-training tokens than contemporaries.

Conclusion: Instella establishes a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research through fully open models and specialized variants.

Abstract: Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.

</details>


### [8] [Black-Box On-Policy Distillation of Large Language Models](https://arxiv.org/abs/2511.10643)
*Tianzhu Ye,Li Dong,Zewen Chi,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.CL

TL;DR: Generative Adversarial Distillation (GAD) enables black-box LLM distillation by framing the student as a generator and training a discriminator to distinguish its outputs from the teacher's, creating a minimax game that provides stable, adaptive feedback.


<details>
  <summary>Details</summary>
Motivation: To enable effective black-box distillation of large language models without access to the teacher model's internal logits or parameters, overcoming limitations of traditional sequence-level knowledge distillation.

Method: GAD frames the student LLM as a generator and trains a discriminator to distinguish between student and teacher responses, creating a minimax game where the discriminator acts as an on-policy reward model that co-evolves with the student.

Result: GAD consistently surpasses sequence-level knowledge distillation, with Qwen2.5-14B-Instruct trained using GAD becoming comparable to its teacher GPT-5-Chat on LMSYS-Chat automatic evaluation.

Conclusion: GAD establishes itself as a promising and effective paradigm for black-box LLM distillation, enabling students to achieve performance comparable to proprietary teacher models.

Abstract: Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.

</details>


### [9] [ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference](https://arxiv.org/abs/2511.10645)
*Yesheng Liang,Haisheng Chen,Song Han,Zhijian Liu*

Main category: cs.CL

TL;DR: ParoQuant is a weight-only post-training quantization method that uses pairwise rotation and channel-wise scaling to handle outliers in LLMs, achieving better accuracy than AWQ with minimal inference overhead.


<details>
  <summary>Details</summary>
Motivation: Current PTQ methods struggle with outliers in weights and activations of LLMs, especially in reasoning tasks where errors accumulate across long chains of thought, leading to accuracy degradation or significant inference overhead.

Method: Combines hardware-efficient independent Givens rotations with channel-wise scaling to equalize magnitude across channels and reduce dynamic range within quantization groups, with co-designed inference kernels for GPU parallelism.

Result: Achieves average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead.

Conclusion: ParoQuant enables more efficient and accurate deployment of reasoning LLMs by effectively handling outliers while maintaining low inference overhead.

Abstract: Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chains of thought. Existing PTQ methods either fail to sufficiently suppress outliers or introduce significant overhead during inference. In this paper, we propose Pairwise Rotation Quantization (ParoQuant), a weight-only PTQ method that combines hardware-efficient and optimizable independent Givens rotations with channel-wise scaling to even out the magnitude across channels and narrow the dynamic range within each quantization group. We further co-design the inference kernel to fully exploit GPU parallelism and keep the rotations and scaling lightweight at runtime. ParoQuant achieves an average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead. This paves the way for more efficient and accurate deployment of reasoning LLMs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [Querying Labeled Time Series Data with Scenario Programs](https://arxiv.org/abs/2511.10627)
*Edward Kim,Devan Shanker,Varun Bharadwaj,Hongbeen Park,Jinkyu Kim,Hazem Torfah,Daniel J Fremont,Sanjit A Seshia*

Main category: cs.AI

TL;DR: The paper addresses the sim-to-real gap in autonomous vehicle testing by introducing a formal method to validate simulated failure scenarios using real-world sensor data through Scenic scenario programs and efficient querying algorithms.


<details>
  <summary>Details</summary>
Motivation: To bridge the sim-to-real gap in CPS testing by ensuring that failure scenarios discovered in simulation environments are reproducible and valid in real-world conditions with actual sensor data.

Method: Developed a formal definition for matching labeled time series sensor data to abstract scenarios using Scenic probabilistic programming language, and created a querying algorithm to identify scenario matches in real datasets.

Result: The algorithm demonstrated higher accuracy and significantly faster performance (orders of magnitude) compared to state-of-the-art commercial vision large language models, with scalability to handle long time series data.

Conclusion: The proposed approach effectively validates simulated failure scenarios against real-world data, providing a reliable method to distinguish between simulation artifacts and genuine system failures, thereby improving the reliability of simulation-based CPS testing.

Abstract: Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber physical systems (CPS). As a result, significant research efforts have been directed toward identifying failure scenarios within simulation environments. However, a critical question remains. Are the AV failure scenarios discovered in simulation reproducible on actual systems in the real world? The sim-to-real gap caused by differences between simulated and real sensor data means that failure scenarios identified in simulation might either be artifacts of synthetic sensor data or actual issues that also occur with real sensor data. To address this, an effective approach to validating simulated failure scenarios is to locate occurrences of these scenarios within real-world datasets and verify whether the failure persists on the datasets. To this end, we introduce a formal definition of how labeled time series sensor data can match an abstract scenario, represented as a scenario program using the Scenic probabilistic programming language. We present a querying algorithm that, given a scenario program and a labeled dataset, identifies the subset of data that matches the specified scenario. Our experiment shows that our algorithm is more accurate and orders of magnitude faster in querying scenarios than the state-of-the-art commercial vision large language models, and can scale with the duration of queried time series data.

</details>
